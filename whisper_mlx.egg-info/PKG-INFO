Metadata-Version: 2.4
Name: whisper-mlx
Version: 0.5.0
Summary: Fast Whisper speech recognition on Apple Silicon with batched decoding
Author: Apple Inc., Mustafa Aljadery, Siddharth Sharma
License: MIT
Project-URL: Homepage, https://github.com/ml-explore/mlx-examples
Project-URL: Documentation, https://github.com/ml-explore/mlx-examples/tree/main/whisper
Project-URL: Repository, https://github.com/ml-explore/mlx-examples
Keywords: whisper,speech recognition,transcription,mlx,apple silicon,machine learning
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: MacOS
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Multimedia :: Sound/Audio :: Speech
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: mlx>=0.11
Requires-Dist: numpy
Requires-Dist: tqdm
Requires-Dist: tiktoken
Requires-Dist: huggingface_hub
Requires-Dist: numba
Requires-Dist: scipy
Provides-Extra: dev
Requires-Dist: pytest; extra == "dev"
Requires-Dist: black; extra == "dev"
Requires-Dist: isort; extra == "dev"
Requires-Dist: mypy; extra == "dev"
Provides-Extra: notebook
Requires-Dist: ipykernel; extra == "notebook"
Requires-Dist: jupyter; extra == "notebook"

# Whisper MLX

Fast Whisper speech recognition on Apple Silicon with batched decoding.

This is a unified implementation combining:
- **ml-explore/mlx-examples/whisper** - Newer APIs, CLI support, output writers, numerical stability
- **lightning-whisper-mlx** - Batched decoding for higher throughput

## Features

- **Batched decoding** - Process multiple audio segments in parallel for 3-5x faster transcription
- **Multiple output formats** - txt, vtt, srt, tsv, json
- **Word-level timestamps** - Extract precise word timings
- **Multiple model support** - tiny, base, small, medium, large-v3, turbo, distil variants
- **Quantization** - 4-bit and 8-bit quantized models for reduced memory usage
- **Simple API** - Easy-to-use `LightningWhisperMLX` wrapper class

## Installation

```bash
# Clone the repository
git clone <repo-url>
cd whisper-mlx

# Install the package
pip install -e .

# Download required assets (mel filters and tokenizer vocabularies)
python -m whisper_mlx.assets.download_assets
```

### Requirements

- macOS with Apple Silicon (M1/M2/M3)
- Python 3.10+
- MLX 0.11+

## Quick Start

### Simple API

```python
from whisper_mlx import LightningWhisperMLX

# Initialize with batched decoding
whisper = LightningWhisperMLX(model="distil-large-v3", batch_size=12)

# Transcribe
result = whisper.transcribe("audio.mp3")
print(result["text"])

# With options
result = whisper.transcribe(
    "audio.mp3",
    language="en",
    word_timestamps=True,
)
```

### Full API

```python
from whisper_mlx import transcribe

result = transcribe(
    "audio.mp3",
    path_or_hf_repo="mlx-community/whisper-turbo",
    batch_size=6,
    language="en",
    word_timestamps=True,
)

print(result["text"])
for segment in result["segments"]:
    print(f"[{segment['start']:.2f} -> {segment['end']:.2f}] {segment['text']}")
```

### CLI

```bash
# Basic transcription
whisper-mlx audio.mp3

# With batched decoding (faster)
whisper-mlx audio.mp3 --batch-size 12

# Specify model and output format
whisper-mlx audio.mp3 --model mlx-community/distil-whisper-large-v3 --output-format srt

# Multiple files
whisper-mlx audio1.mp3 audio2.mp3 --output-dir ./transcripts

# With word timestamps
whisper-mlx audio.mp3 --word-timestamps True

# Translate to English
whisper-mlx audio.mp3 --task translate
```

## Available Models

| Model | HuggingFace Repo | Size | Speed |
|-------|------------------|------|-------|
| tiny | mlx-community/whisper-tiny-mlx | 39M | Fastest |
| base | mlx-community/whisper-base-mlx | 74M | Fast |
| small | mlx-community/whisper-small-mlx | 244M | Medium |
| medium | mlx-community/whisper-medium-mlx | 769M | Slow |
| large-v3 | mlx-community/whisper-large-v3-mlx | 1.5B | Slowest |
| turbo | mlx-community/whisper-turbo | 809M | Fast |
| distil-large-v3 | mlx-community/distil-whisper-large-v3 | 756M | Fast |

### Quantized Models

For reduced memory usage, use quantized models:

```python
whisper = LightningWhisperMLX(model="distil-large-v3", quant="4bit")
```

## Batch Size Recommendations

| Model | Recommended batch_size | Memory Usage |
|-------|------------------------|--------------|
| tiny/base | 24-32 | Low |
| small | 16-24 | Medium |
| medium | 8-12 | High |
| large/turbo | 4-8 | High |
| distil-large-v3 | 12-16 | Medium |

Higher batch sizes improve throughput but require more memory. Start with the recommended values and adjust based on your hardware.

## API Reference

### transcribe()

```python
def transcribe(
    audio: Union[str, np.ndarray, mx.array],
    *,
    path_or_hf_repo: str = "mlx-community/whisper-turbo",
    batch_size: int = 1,
    verbose: Optional[bool] = None,
    temperature: Union[float, Tuple[float, ...]] = (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),
    compression_ratio_threshold: Optional[float] = 2.4,
    logprob_threshold: Optional[float] = -1.0,
    no_speech_threshold: Optional[float] = 0.6,
    condition_on_previous_text: bool = True,
    initial_prompt: Optional[str] = None,
    word_timestamps: bool = False,
    **decode_options,
) -> dict
```

### LightningWhisperMLX

```python
class LightningWhisperMLX:
    def __init__(
        self,
        model: str = "distil-large-v3",
        batch_size: int = 12,
        quant: str = None,
    )

    def transcribe(
        self,
        audio_path: str,
        language: str = None,
        task: str = "transcribe",
        verbose: bool = False,
        word_timestamps: bool = False,
        **kwargs,
    ) -> dict
```

## License

MIT License - see LICENSE file for details.

## Credits

- [ml-explore/mlx-examples](https://github.com/ml-explore/mlx-examples) - MLX Whisper implementation
- [mustafaaljadery/lightning-whisper-mlx](https://github.com/mustafaaljadery/lightning-whisper-mlx) - Batched decoding
- [OpenAI Whisper](https://github.com/openai/whisper) - Original Whisper model
